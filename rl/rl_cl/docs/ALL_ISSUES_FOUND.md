# All Issues Found - Comprehensive Investigation

**Date**: December 30, 2025
**Training Results**: Stage 1: 17cm, Stage 2: 58.46cm, Stage 3: 235.59cm

---

## ðŸš¨ CRITICAL ISSUES IDENTIFIED

### Issue #1: TMD Stiffness Too Soft for Active Control

**Severity**: CRITICAL
**Status**: âŒ IDENTIFIED, NOT FIXED

**Problem**:
- TMD stiffness: 3,765 N/m (optimized for passive performance)
- This is TOO SOFT for active control forces (50-150 kN)
- Causes TMD displacement runaway: 867 cm (Stage 1) â†’ 6,780 cm (Stage 3)
- Observation clipping: 85-99% of timesteps have clipped observations

**Evidence**:
```
Stage 1 (50 kN max force):
  TMD displacement: -0.026 to +8.677 m (bounds: Â±1.5 m)
  Exceeds bounds by 5.8x
  Clipping: 84.6% of timesteps

Stage 2 (100 kN max force):
  TMD displacement: -10.096 to +0.000 m
  Exceeds bounds by 6.7x
  Clipping: 97.3% of timesteps

Stage 3 (150 kN max force):
  TMD displacement: +0.000 to +67.799 m (!!!)
  Exceeds bounds by 45x
  Clipping: 99.8% of timesteps
```

**Root Cause**:
- We optimized TMD for passive performance (frequency ratio = 0.80)
- This made TMD very soft (k = 3,765 N/m)
- Soft TMD works for passive (small inertial forces)
- But fails with active control (large external forces)

**Solution**: Increase TMD stiffness to 50,000 N/m (original value)

**See**: [INVESTIGATION_FINDINGS.md](INVESTIGATION_FINDINGS.md) for full analysis

---

### Issue #2: DCR Reward Explodes in Early Steps

**Severity**: CRITICAL
**Status**: âŒ IDENTIFIED, NOT FIXED

**Problem**:
- DCR penalty **dominates** reward signal completely
- Early in episode, DCR reaches absurd values (7,000+)
- This produces massive negative penalties (-100 million!)
- Destroys ability to learn useful policy

**Evidence**:
```
Average reward components (first 20 steps, no control):
  dcr            : -7,593,871.29 (100.0%)  â† DOMINATES EVERYTHING
  acceleration   :      -0.01 (  0.0%)
  velocity       :      -0.01 (  0.0%)
  displacement   :      -0.00 (  0.0%)
  force          :       0.00 (  0.0%)
  smoothness     :       0.00 (  0.0%)
```

**Step-by-step breakdown**:
```
Step  0: DCR=7,269.31  Penalty=-105,656,517.63  â† ABSURD!
Step  1: DCR=3,644.25  Penalty= -26,546,521.40
Step  2: DCR=1,042.96  Penalty=  -2,171,374.49
Step  3: DCR=  397.48  Penalty=    -314,399.25
...
Step 19: DCR=   15.95  Penalty=        -446.96
Step 50: DCR=    1.02  Penalty=          -0.00  â† Normal
```

**Root Cause**:
At the very start of an episode:
1. Only 1-2 floors have experienced any drift
2. Max drift might be 0.006 cm (floor 1)
3. 75th percentile might be 0.000001 cm (floors 9-12 haven't moved yet)
4. **DCR = 0.006 / 0.000001 = 6,000!**
5. Penalty = -2.0 Ã— (6000 - 1)Â² = -71,988,000!

**Why This Breaks Training**:
- Agent gets massive negative reward in first 10-20 steps
- This happens EVERY episode
- Agent learns: "everything is terrible no matter what I do"
- Cannot distinguish between good and bad actions
- Reward signal is 100% noise from spurious early DCR

**Current Code** (lines 384-396):
```python
if np.max(self.peak_drift_per_floor) > 1e-10:
    sorted_peaks = np.sort(self.peak_drift_per_floor)
    percentile_75 = np.percentile(sorted_peaks, 75)
    max_peak = np.max(self.peak_drift_per_floor)

    if percentile_75 > 1e-10:  # â† This threshold is TOO LOW!
        current_dcr = max_peak / percentile_75  # â† Can be 10,000+!
        dcr_deviation = max(0, current_dcr - 1.0)
        dcr_penalty = -2.0 * (dcr_deviation ** 2)  # â† Squared makes it worse!
```

**Solution Options**:

**Option A: Higher threshold for DCR calculation** (RECOMMENDED)
```python
# Only calculate DCR after sufficient drift has occurred
if percentile_75 > 0.001:  # 1mm minimum drift (was 0.00001mm!)
    current_dcr = max_peak / percentile_75
    dcr_deviation = max(0, current_dcr - 1.0)
    dcr_penalty = -2.0 * (dcr_deviation ** 2)
else:
    dcr_penalty = 0.0  # Don't penalize DCR early in episode
```

**Option B: Clip DCR to reasonable range**
```python
if percentile_75 > 1e-10:
    current_dcr = max_peak / percentile_75
    current_dcr = np.clip(current_dcr, 0.0, 10.0)  # Cap at 10x max
    dcr_deviation = max(0, current_dcr - 1.0)
    dcr_penalty = -2.0 * (dcr_deviation ** 2)
```

**Option C: Use mean drift instead of percentile**
```python
# Mean is more stable than percentile early in episode
mean_drift = np.mean(self.peak_drift_per_floor)
max_drift = np.max(self.peak_drift_per_floor)

if mean_drift > 0.001:  # 1mm minimum
    current_dcr = max_drift / mean_drift
    dcr_deviation = max(0, current_dcr - 1.0)
    dcr_penalty = -2.0 * (dcr_deviation ** 2)
else:
    dcr_penalty = 0.0
```

**Option D: Delay DCR calculation until sufficient steps**
```python
# Only calculate DCR after 100 steps (2 seconds)
if self.current_step > 100 and percentile_75 > 1e-10:
    current_dcr = max_peak / percentile_75
    dcr_deviation = max(0, current_dcr - 1.0)
    dcr_penalty = -2.0 * (dcr_deviation ** 2)
else:
    dcr_penalty = 0.0
```

**Recommended**: **Option A** (higher threshold) - Simple, robust, physically meaningful

---

## âœ… VERIFIED CORRECT (No Issues)

### Environment Configuration
- âœ… Earthquake data integrity: Final accel = 0.0, consistent timesteps
- âœ… Mass matrix: Diagonal, correct values (200,000 kg floors, 4,000 kg TMD)
- âœ… Stiffness matrix: Symmetric, TMD coupling correct (-3,765 N/m)
- âœ… Damping matrix: Symmetric, Rayleigh damping correctly applied
- âœ… Newmark integration: Unconditionally stable (Î²=0.25, Î³=0.5)

### Observation and Action Spaces
- âœ… Observation space: 8 values, correctly defined
- âœ… Action space: [-1, +1] correctly maps to Â±max_force
- âœ… Training/inference match: Model and environment spaces identical
- âœ… No NaN or Inf values during simulation

### Passive TMD Observations (Without Active Control)
- âœ… All building observations stay within bounds
- âœ… Only TMD observations exceed bounds (but only with active control!)

### Domain Randomization
- âœ… Sensor noise: Correctly applied
- âœ… Actuator noise: Correctly applied
- âœ… Latency: Correctly buffered
- âœ… Dropout: Correctly applied
- âœ… Provides robustness (< 20% degradation)

---

## Summary Table

| Issue | Severity | Impact | Status | Fix Required |
|-------|----------|--------|--------|--------------|
| **TMD Too Soft** | ðŸ”´ CRITICAL | TMD runaway, 85-99% obs clipping | âŒ Not fixed | Increase k to 50,000 N/m |
| **DCR Explodes** | ðŸ”´ CRITICAL | Reward signal destroyed (100M penalty!) | âŒ Not fixed | Higher threshold (0.001 m) |
| Earthquake data | âœ… OK | None | âœ… Correct | None |
| Mass matrix | âœ… OK | None | âœ… Correct | None |
| Stiffness matrix | âœ… OK | None | âœ… Correct | None |
| Damping matrix | âœ… OK | None | âœ… Correct | None |
| Observation space | âœ… OK | None | âœ… Correct | None |
| Action space | âœ… OK | None | âœ… Correct | None |
| Domain randomization | âœ… OK | None | âœ… Correct | None |

---

## Impact Analysis

### Why Training Produced Suboptimal Results

**Stage 1 (17 cm vs 15-18 cm expected)**: Actually GOOD!
- Peak roof displacement: 17.70 cm âœ…
- Within expected range
- **BUT**: TMD displacement was 867 cm (clipped)
- **AND**: DCR reward exploded early in every episode

**Stage 2 (58 cm vs 30-35 cm expected)**: WORSE than Stage 1
- Peak roof displacement: 58.46 cm âŒ
- TMD displacement: 1,009 cm (clipped 97% of time)
- DCR reward even worse with larger earthquake
- Agent couldn't learn due to:
  1. Corrupted observations (clipping)
  2. Destroyed reward signal (DCR explosion)

**Stage 3 (236 cm vs 180-200 cm expected)**: MUCH WORSE
- Peak roof displacement: 235.59 cm âŒ
- TMD displacement: 6,780 cm (67 meters!) (clipped 99.8% of time)
- DCR penalties in millions
- Agent completely blind (all observations clipped)
- Agent completely confused (reward is noise)

### Combined Effect

The two bugs **compound each other**:
1. TMD runs away â†’ observations clipped â†’ agent sees wrong state
2. DCR explodes â†’ reward destroyed â†’ agent can't learn
3. Agent applies more force (trying to help) â†’ TMD runs away more
4. Vicious cycle!

---

## Recommended Fix Order

**CRITICAL: Both issues must be fixed together before retraining!**

### Step 1: Fix TMD Stiffness
```python
# In tmd_environment.py and rl_cl_tmd_environment.py (lines 65-66)
self.tmd_k = 50000   # N/m (was 3765)
self.tmd_c = 2000    # NÂ·s/m (was 194)
```

### Step 2: Fix DCR Reward Calculation
```python
# In tmd_environment.py (lines 389-396)
# Option A: Higher threshold
if percentile_75 > 0.001:  # 1mm minimum (was 1e-10)
    current_dcr = max_peak / percentile_75
    dcr_deviation = max(0, current_dcr - 1.0)
    dcr_penalty = -2.0 * (dcr_deviation ** 2)
else:
    dcr_penalty = 0.0
```

### Step 3: Delete Old Models
```bash
rm -rf rl_cl_robust_models/*
```

### Step 4: Retrain
```bash
python train_robust_rl_cl.py --earthquakes <files>
```

---

## Expected Results After Fixes

**Stage 1 (M4.5)**:
- Peak roof displacement: 15-18 cm
- TMD displacement: < 50 cm (within Â±150 cm bounds)
- DCR reward: -10 to -50 (reasonable)
- Observation clipping: < 5%

**Stage 2 (M5.7)**:
- Peak roof displacement: 30-35 cm
- TMD displacement: < 80 cm
- DCR reward: -20 to -100 (reasonable)
- Observation clipping: < 10%

**Stage 3 (M7.4)**:
- Peak roof displacement: 180-200 cm
- TMD displacement: < 150 cm (at bound, but not exceeding)
- DCR reward: -100 to -500 (higher but not explosive)
- Observation clipping: < 20%

---

## Files That Need Changes

### 1. Training Environment
- **File**: `rl/rl_cl/tmd_environment.py`
- **Lines 65-66**: TMD stiffness and damping
- **Lines 389-396**: DCR reward calculation

### 2. API Environment
- **File**: `restapi/rl_cl/rl_cl_tmd_environment.py`
- **Lines 65-66**: TMD stiffness and damping
- **Lines 389-396**: DCR reward calculation (if it exists)

---

## Status

ðŸ”´ **TWO CRITICAL BUGS IDENTIFIED - AWAITING USER APPROVAL TO FIX**

Both bugs must be fixed before retraining will produce good results!

**Confidence**: 100% - Both bugs verified through systematic investigation with clear evidence.
